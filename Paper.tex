%\documentclass[a4paper,english,12pt,twocolumn]{article}
\documentclass[a4paper,english,12pt, twocolumn]{article}
\usepackage[utf8]{inputenc} % Encodage du fichier
\usepackage[T1]{fontenc} % Encodage des fonts nécessaire pour le Latin
\usepackage[french]{babel} % Pour changer la langue des mots générés et choisir la bonne mise en page
\usepackage{amssymb}
\usepackage{pdflscape}
\usepackage{microtype} 
\usepackage{lmodern} % Le latin modèrne
\usepackage[top=1.5cm, bottom=1.5cm, left=2.5cm, right=1.5cm]{geometry} % Définir les marges de la page 
\usepackage[hidelinks,urlcolor=blue,unicode=true,
pdftitle={Understanding Deep Learning requires rethinking generalization},
pdfauthor={BELOUADAH Eden and BOUHAHA Mariem},
pdfdisplaydoctitle=true]{hyperref} % Pour les liens 
\usepackage{fancyhdr} % Pour le style de la page
\usepackage[font=it]{caption} % Rendre les titres des tableaux italiques
\usepackage{graphicx} % Pour les images
\usepackage{subcaption} % Pour mettre plusieurs images sur la même ligne
\usepackage{float} % Pour empêcher le déplacement des tableaux et des figures.
\usepackage{babelbib} % Pour changer la langue dans la bibliographie
\usepackage{amsmath} % Pour des fonctions mathématiques
\usepackage{amssymb} % Pour les symboles mathématiques
%\usepackage[onelanguage,english,longend,boxruled,algoruled,linesnumbered,algochapter,nofillcomment]{algorithm2e} %pour les algorithmes
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{setspace}

\graphicspath{ {photos/} } % Spécifier le répertoire contenant les images

\DisableLigatures[f]{encoding=*}

%Active ça si tu ne veux pas les points-virgules dans les algorithmes
% \DontPrintSemicolon
 
%\renewcommand \thechapter{\Roman{chapter}} % Utiliser les numéros romans pour les chapitres

\captionsetup{labelfont=it,textfont=it,labelsep=period} % Changer le style des légendes
\AtBeginDocument{ % Changer les légendes
	\renewcommand\tablename{\itshape Tableau}
	\renewcommand{\figurename}{\itshape Figure}
	% Renommer la table des matières
	\renewcommand{\contentsname}{Sommaire}
}

% Style de l'entête et le pied de la page
\setlength{\headheight}{-60pt}
\pagestyle{fancy}
\fancyhead[L]{} % Enlever la section
\fancyhead[R]{\footnotesize\slshape{\nouppercase{\leftmark}}} % Titre du chapitre en minuscule avec taille 10
\fancyfoot[C]{}
\fancyfoot[R]{\thepage} % Déplacer le numéro de la page vers la droite de la page

\fancypagestyle{plain}{
\renewcommand{\headrulewidth}{0pt}
\fancyhf{}
\fancyfoot[R]{\thepage}
}
  
% Espace entre les lignes
\linespread{1.3}

% Code pris de https://tex.stackexchange.com/a/95616/109916 et corrigé
% Début
\makeatletter
\newcommand{\emptypage}[1]{
  \cleardoublepage
  \begingroup
  \let\ps@plain\ps@empty
  \pagestyle{empty}
  #1
  \cleardoublepage
  \endgroup}
\makeatletter
% Fin


% pour changer les deux points des légendes d'algorithmes
% \SetAlgoCaptionSeparator{\unskip.}

\begin{document}
%\include{Page_de_garde}
%\include{Remerciements}
\emptypage{
%\tableofcontents
%\listoffigures
%\listoftables
}
    
\setlength{\parskip}{0.6em plus 0.1em minus 0.1em}
%\SetKwInput{KwOut}{Outpits}

% Redéfinition des chapitres et sections pour les inclure dans le sommaire
\makeatletter
%	\let\oldchapter\chapter
%	\newcommand{\@chapterstar}[1]{\cleardoublepage\phantomsection\addcontentsline{toc}{chapter}{#1}{\oldchapter*{#1}}\markboth{#1}{}}
%	\newcommand{\@chapternostar}[1]{{\oldchapter{#1}}}
%	\renewcommand{\chapter}{\@ifstar{\@chapterstar}{\@chapternostar}}
\let\oldsection\section
\newcommand{\@sectionstar}[1]{\phantomsection\addcontentsline{toc}{section}{#1}{\oldsection*{#1}}}
\newcommand{\@sectionnostar}[1]{{\oldsection{#1}}}
\renewcommand\section{\@ifstar{\@sectionstar}{\@sectionnostar}}	
\newcommand*{\rom}[1]{\expandafter\@slowromancap\romannumeral #1@}
\makeatother

\setcounter{page}{1}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{\Large Understanding Deep Learning Requires Rethinking Generalization \\Présentation et analyse}
\author{\normalsize Eden BELOUADAH \and \normalsize Mariem BOUHAHA}
\date{}
\maketitle
\section{Introduction}
L'article \emph{Understanding Deep Learning Requires Rethinking Generalization} a été publié en Novembre 2016 dans arXiv par Chiyuan Zhang (MIT), Samy Bengio (Google Brain), Moritz Hardt (Google Brain), Benjamin Recht (UC) et Oriol Vinyals (DeepMind). Dans cet article, les auteurs répondent à la question suivante:

\textit{Par quoi expliquer la bonne généralisation des Réseaux de Neurones en pratique? }

Selon les auteurs, répondre à cette question va permettre de rendre les réseaux de neurones plus interprétables et aboutir à un design plus fiable de l’architecture du modèle. 

\section{Contributions}
A travers plusieurs expérimentations, les auteurs montraient comment les approches traditionnelles échouent dans l’explication du phénomène que les réseaux de neurones généralisent bien en pratique. Par “généraliser bien” les auteurs voulaient simplement dire “Qu’est-ce qui cause un réseau de neurones qui performe bien sur les données d’apprentissage à performer aussi bien sur les données de test ? » 
En expérimentant sur les data sets CIFAR 10 (50.000 images d’apprentissage, 10.000 images de validation, 10 classes) et ImageNet (1.281.167 000 images d’apprentissage, 50,000 images de validation, 1000 classes) et en se basant sur l’architecture Inception V3 et une de ses versions (AlexNet) ainsi qu'en utilisant des MLPs, les auteurs trouvaient que :
\begin{itemize}
	\item les réseaux de neurones apprennent facilement des labels aléatoires. 
	\item La régularisation “explicite” peut améliorer la généralisation du réseau de neurones, mais elle n’est pas nécessaire ni suffisante pour contrôler l’erreur de généralisation. 
	\item Ils complétaient leur approche expérimentale par une construction théorique montrant qu'il existe un réseau de neurones à 2 couches avec des activations ReLU et 2n+d poids qui peut représenter n’importe quelle fonction sur un échantillon de données de taille n et de dimension d. 
\end{itemize}

Et en faisant appel aux modèles linéaires, les auteurs analysaient comment la Descente de Gradient Stochastique (SGD) agit comme une régularisation « implicite » et suggéraient qu’une investigation plus approfondie devrait être faite pour comprendre les propriétés des modèles qui utilisent la SGD dans leur apprentissage.

\vspace{-2em}

\section{Analyse}
\vspace{-1em}

Selon la Théorie d’Apprentissage Statistique, un modèle généralise bien s’il n’a pas la capacité d’overfitter des données aléatoires de même taille que les données d’apprentissage réelles. 
Dans cet article, les auteurs discutaient de la capacité des réseaux de neurones et des méthodes de régularisation les plus utilisées en Machine Learning et trouvaient que la théorie classique d’apprentissage statistique ainsi que les stratégies de régularisation ne peuvent pas expliquer la remarquable capacité des réseaux de neurones à bien se généraliser. 

Cet article présente un ensemble d’expérimentations, dont \textbf{la randomisation des labels} et \textbf{l’ajout de bruit}, qui ont mis en relief le pouvoir énorme des réseaux de neurones larges. Encore, ces mêmes modèles arrivent à généraliser même quand on supprime toute forme de \textbf{régularisation explicite} ou \textbf{implicite}. Ces observations sont utilisées pour montrer l’incapacité de la théorie classique (dimension de VC, complexité de Rademacher, stabilité uniforme) à expliquer le pouvoir de généralisation.  

Comme l’ont mentionné les auteurs, dès qu’une famille de modèles arrive à mémoriser toutes les données d’apprentissage, la théorie classique ne permet pas de percevoir le comportement de ces modèles en termes de généralisation, ce qui ne laisse que le choix d’expérimentation et d’étude empirique.

Bien que ce travail ne propose pas assez d’explications pour ces capacités de généralisation, il oblige le lecteur à penser au problème de généralisation d’un nouvel angle, différemment de la façon traditionnelle dont on le comprend. 
\vspace{-1em}
\section{Critiques}
\vspace{-1em}
D'abord, l’article montre qu’une bonne généralisation n’est pas souhaitée pour un problème à labels aléatoires. Ceci est certainement correct, mais nous pensons que c’est inintéressant en pratique, du moment où on n'a pas intérêt à travailler avec des étiquettes aléatoires, pour une finalité de classification par exemple.  
Ensuite, bien que l’article incite à la réflexion, à la lecture et à la discussion, on n’a cependant pas très bien clarifié de quelles “approches traditionnelles” parlait-on et pourquoi est-il si surprenant qu’elles échouent à expliquer la performance des réseaux de neurones. Les auteurs expriment leur surprise à propos de certaines capacités des réseaux de neurones, mais ils ne donnaient pas de références ni d’arguments qui expliquent pourquoi devrait-on être surpris. 
On s'est surpris aussi du fait que la SGD arrive à optimiser les poids pour fitter les étiquettes aléatoires sans aucun changement d'hyper paramètres. On sait tous que les réseaux de neurones larges ont suffisamment de paramètres pour mémoriser les étiquettes aléatoires, donc ce n’est pas si évident que ce soit surprenant. 
Enfin, il aurait été mieux de présenter des explications alternatives au phénomène de généralisation, au lieu de seulement remettre en question  les approches traditionnelles.
\vspace{-1em}
\section{Conclusion} 
\vspace{-1em}
Les expérimentations présentées dans cet article mettent l’accent sur l’importance de comprendre la capacité effective des réseaux de neurones, qui sont capables de mémoriser facilement tout l’ensemble d’apprentissage, même lorsqu'on casse le lien entre les données et les étiquettes. Une optimisation empirique facile n'est à priori pas la vraie raison de généralisation. C’est pour cela qu’il est nécessaire de comprendre les vraies notions qui se cachent derrière la généralisation pour pouvoir bien comprendre l’apprentissage profond.
\vspace{-1em}
\section*{Références}
\vspace{-1em}
[1] Zhang, C., Bengio, S., Hardt, M., Recht, B. and Vinyals, O., 2016. Understanding deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\bibliographystyle{babplain}
%\parskip=-1em
%\emptypage{\bibliography{bibliographie}}
%\let\section\oldsection % pour éviter que le résumé soient visibles dans le sommaire comme une section
%\include{Resume}
\end{document}
